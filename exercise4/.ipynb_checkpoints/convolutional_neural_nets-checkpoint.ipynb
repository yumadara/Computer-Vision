{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "In this exercise you will be introduced to some practical aspects of deep learning in\n",
    "computer vision, including constructing a deep neural network and training it via gradient\n",
    "descent to tackle image classification.\n",
    "\n",
    "We will use the popular TensorFlow framework through the Keras API.\n",
    "\n",
    "We will tackle **image classification** through deep learning methods, in particular we will look at\n",
    "\n",
    "* Dataset download and normalization\n",
    "* Softmax regression with stochastic gradient descent and Adam\n",
    "* Multilayer perceptrons with tanh and ReLU\n",
    "* A basic convolutional net\n",
    "* BatchNorm, striding, global average pooling\n",
    "* Residual networks\n",
    "* Learning rate decay and data augmentation\n",
    "\n",
    "\n",
    "### Install TensorFlow\n",
    "\n",
    "Install TensorFlow using `pip install tensorflow`. TensorFlow can use GPUs to make the training several times faster, but since not all of you may have access to a GPU, we have tried to scale this exercise with a CPU in mind. For more info on installing TensorFlow, see https://www.tensorflow.org/install\n",
    "\n",
    "### TensorBoard Plotting\n",
    "\n",
    "TensorBoard is a web-based tool for drawing pretty plots of quantities we care about during training, such as the loss. We need to choose a folder where these values will be stored (\"logdir\").\n",
    "\n",
    "Start the TensorBoard server by executing e.g. `tensorboard --logdir /tmp/tensorboard_logs` after you've activated your conda environment. If you change the logdir, also adjust it in the cell below.\n",
    "\n",
    "You can view the graphs by visiting http://localhost:6006 in your browser (6006 is the default port).\n",
    "At first there will be nothing to plot, so it will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_root= '/tmp/tensorboard_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- Run this cell to add heading letters per subtask (like a, b, c) -->\n",
    "<style>\n",
    "body {counter-reset: section;}\n",
    "h2:before {counter-increment: section;\n",
    "           content: counter(section, lower-alpha) \") \";}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import cv2\n",
    "from attrdict import AttrDict\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "import tensorflow.keras.initializers as initializers\n",
    "import tensorflow.keras.preprocessing.image as kerasimage\n",
    "\n",
    "# Just an image plotting function\n",
    "def plot_multiple(images, titles=None, colormap='gray',\n",
    "                  max_columns=np.inf, imwidth=4, imheight=4, share_axes=False):\n",
    "    \"\"\"Plot multiple images as subplots on a grid.\"\"\"\n",
    "    if titles is None:\n",
    "        titles = [''] *len(images)\n",
    "    assert len(images) == len(titles)\n",
    "    n_images = len(images)\n",
    "    n_cols = min(max_columns, n_images)\n",
    "    n_rows = int(np.ceil(n_images / n_cols))\n",
    "    fig, axes = plt.subplots(\n",
    "        n_rows, n_cols, figsize=(n_cols * imwidth, n_rows * imheight),\n",
    "        squeeze=False, sharex=share_axes, sharey=share_axes)\n",
    "\n",
    "    axes = axes.flat\n",
    "    # Hide subplots without content\n",
    "    for ax in axes[n_images:]:\n",
    "        ax.axis('off')\n",
    "        \n",
    "    if not isinstance(colormap, (list,tuple)):\n",
    "        colormaps = [colormap]*n_images\n",
    "    else:\n",
    "        colormaps = colormap\n",
    "\n",
    "    for ax, image, title, cmap in zip(axes, images, titles, colormaps):\n",
    "        ax.imshow(image, cmap=cmap)\n",
    "        ax.set_title(title)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "We are going to tackle the classic image classification task using the **CIFAR-10 dataset**, containing 60,000 32x32 RGB images of 10 different classes (10,000 for training and 10,000 for testing). \n",
    "\n",
    "![image.png](cifar.png)\n",
    "\n",
    "The dataset is automatically downloaded if you run the next cell.\n",
    "You may read more about the dataset at https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "\n",
    "A common normalization strategy is to map the image RGB values to the range 0-1 and to subtract the mean training pixel value. Perform this normalization below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "675ff7325ec1b28c7cfd9735b49eb79f",
     "grade": true,
     "grade_id": "cell-ea360dbfd9ffa26b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "(im_train, y_train), (im_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize to 0-1 range and subtract mean of training pixels\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "image_shape = x_train[0].shape\n",
    "labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regression\n",
    "\n",
    "Before considering convolutional neural networks, let us start with a simpler classifier called softmax regression (a.k.a. multinomial logistic regression). Note that even though the name contains \"regression\", this is a classification model.\n",
    "\n",
    "Softmax regression can be understood as a single-layer neural network. We first flatten our input image to a long vector $\\mathbf{x}$, consisting of $32\\cdot 32\\cdot 3= 3072$ values. Then we predict class probabilities $\\hat{\\mathbf{y}}$ through a fully-connected layer with softmax activation:\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = W \\mathbf{x} + \\mathbf{b} \\\\\n",
    "\\hat{y}_c = \\operatorname{softmax}(\\mathbf{z})_c = \\frac{\\exp{z_c}}{\\sum_{\\tilde{c}=1}^{10} \\exp{z_{\\tilde{c}}}}\n",
    "$$\n",
    "\n",
    "Here $z_c$ denotes the $c$th component of the vector $\\mathbf{z}$, called the vector of **logits**.\n",
    "The weights $W$ and biases $\\mathbf{b}$ will be learned during training.\n",
    "\n",
    "### Training\n",
    "\n",
    "We train the model by minimizing a **loss function** averaged over the training data. As we are tackling a classification problem, the **cross-entropy** is a suitable loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CE}(\\mathbf{y}, \\hat{\\mathbf{y}}; W, \\mathbf{b}) = - \\sum_{c=1}^{10} y_c \\log{\\hat{y}_c}\n",
    "$$\n",
    "\n",
    "Note that in the above notation the ground-truth $\\mathbf{y}$ is a so-called **one-hot vector**, containing a single 1 component, while the remaining components \n",
    "are zeros. The model's predicted $\\hat{\\mathbf{y}}$ is a vector which also sums to one, but whose components all take continuous values in the range $(0, 1)$. What is the intuition behind this loss function?\n",
    "\n",
    "We minimize the loss by **stochastic gradient descent** (SGD). That is, we repeatedly sample mini-batches from the training data and update the parameters (weights and biases) towards the direction of the steepest decrease of the loss averaged over the mini-batch. For example, the weight $w_{ij}$ (an element of the matrix $W$) is updated according to:\n",
    "\n",
    "$$\n",
    "w_{ij}^{(t+1)} = w_{ij}^{(t)} - \\eta \\cdot \\frac{\\partial \\mathcal{L}_{CE}} {\\partial w_{ij}},\n",
    "$$\n",
    "\n",
    "with $\\eta$ being the learning rate.\n",
    "\n",
    "----\n",
    "\n",
    "This is all very straightforward to perform in Keras. `models.Sequential` accepts a list of layers that are applied sequentially, in a chain. Here we have two layers, `Flatten` to convert the image into a long vector and `Dense`, which is a synonym for fully-connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_regression = models.Sequential([\n",
    "    layers.Flatten(input_shape=image_shape),\n",
    "    layers.Dense(10, activation='softmax')],\n",
    "    name='linear')\n",
    "\n",
    "def train_model(model, batch_size=128, n_epochs=30,  optimizer=optimizers.SGD, learning_rate=1e-2):\n",
    "    opt = optimizer(lr=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    logdir = os.path.join(log_root, f'{model.name}_{timestamp}')\n",
    "    tensorboard_callback = callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "    model.fit(x=x_train, y=y_train, verbose=1, epochs=n_epochs, \n",
    "              validation_data=(x_test, y_test), batch_size=batch_size,\n",
    "              callbacks=[tensorboard_callback])\n",
    "\n",
    "train_model(softmax_regression, optimizer=optimizers.SGD, learning_rate=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Jupyter Notebook Tip: After you're done training, you can collapse or hide the output by clicking or double clicking the area directly to the left of the output.)\n",
    "\n",
    "You can check the how the loss and accuracy change over the course of trainng in TensorBoard.\n",
    "\n",
    "What would be the cross-entropy loss for a dummy classifier that always outputs equal probabilities for all the classes? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer\n",
    "\n",
    "There has been a lot of research on improving on the simple stochastic gradient descent algorithm we used above. One of the most popular variants is called **Adam** (https://arxiv.org/abs/1412.6980, \"adaptive moment estimation\"). Its learning rate usually requires less precise tuning, and something in the range of $(10^{-4},10^{-3})$ often works well in practice. Intuitively, this is because the algorithm automatically adapts the learning rate for each weight depending on the gradients.\n",
    "\n",
    "You can run it as follows (the optimizer is passed to Keras's `model.fit` function in `train_model`). The difference is not large for such a simple model, but makes a bigger difference for larger networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_regression = models.Sequential([\n",
    "    layers.Flatten(input_shape=image_shape),\n",
    "    layers.Dense(10, activation='softmax')],\n",
    "    name='linear')\n",
    "train_model(softmax_regression, optimizer=optimizers.Adam, n_epochs=30, learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Learned Weights\n",
    "\n",
    "Multiplication by the weights $W$ can be interpreted as computing responses to correlation templates per image class.\n",
    "\n",
    "That means, we can reshape the weight array $W$ to a obtain \"template images\".\n",
    "\n",
    "Perform this reshaping and visualize the resulting templates.\n",
    "Do they look as you would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0476ad79fedcc7f272e0656add2253f2",
     "grade": true,
     "grade_id": "cell-6b90988a89171165",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "W, b = softmax_regression.layers[1].get_weights()\n",
    "\n",
    "# Create the `templates` variable here based on W, with dimensions [10 (num_classes), height, width, 3 (rgb)]\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# We normalize the templates to the 0-1 range for visualization\n",
    "mini = np.min(templates, axis=(1,2,3), keepdims=True)\n",
    "maxi = np.max(templates, axis=(1,2,3), keepdims=True)\n",
    "rescaled_templates = (templates - mini)/ (maxi-mini)\n",
    "plot_multiple(rescaled_templates, labels, max_columns=5, imwidth=1, imheight=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "Softmax regression has a big limitation: the decision surface between any two classes (i.e. the part of the input space where the classification decision changes from one class to another) is a simple hyperplane (\"flat\").\n",
    "\n",
    "The **multi-layer perceptron** (MLP) is a neural network model with additional layer(s) between the input and the logits (so-called hidden layers), with nonlinear activation functions. Why are activation functions needed?\n",
    "\n",
    "Before the current generation of neural networks, the **hyperbolic tangent** (tanh) function used to be the preferred activation function in the hidden layers of MLPs. It is sigmoid shaped and has a range of $(-1,1)$. We can create such a network in Keras as follows. Does it obtain better results than the linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_mlp = models.Sequential([\n",
    "    layers.Flatten(input_shape=image_shape),\n",
    "    layers.Dense(512, activation='tanh'),\n",
    "    layers.Dense(10, activation='softmax')],\n",
    "    name='tanh_mlp')\n",
    "\n",
    "train_model(tanh_mlp, optimizer=optimizers.Adam, learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "The ReLU activation function has become more popular in recent years, especially for deeper nets. Create and train an MLP that uses ReLU as the activation. Do the results change? What benefits does ReLU have against tanh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05d6ae30fdb61c0ac00b0606853fb3dc",
     "grade": true,
     "grade_id": "cell-e96f4ec268fbbe6c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_mlp = models.Sequential([\n",
    "    layers.Flatten(input_shape=image_shape),\n",
    "    layers.Dense(512, activation='relu', kernel_initializer='he_uniform'),\n",
    "    layers.Dense(10, activation='softmax')],\n",
    "    name='relu_mlp')\n",
    "train_model(relu_mlp, optimizer=optimizers.Adam, learning_rate=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Convolutional Neural Network\n",
    "\n",
    "The previous models did not explicitly make use of the grid structure of the image pixels. Convolutional neural networks do.\n",
    "\n",
    "Instead of reshaping the input image pixels into one long vector, convolutional layers slide small filters across the input, just as with the convolutional filters we saw earlier in the course. In the earlier parts, we looked at convolution on an image with a single channel in case of grayscale images, or channelwise separate convolutions on RGB images.\n",
    "\n",
    "In CNNs, the multiple input channels of a conv layer are not handled independently, but are linearly combined. This means that the weight array has shape `[kernel_height, kernel_width, num_input_channels, num_output_channels]` and we perform a weighted sum along the input channel axis. Another difference is the use of a **bias** vector of shape `[num_output_channels]`, each component of which gets added on the corresponding output channel.\n",
    "\n",
    "As you already know, convolution is a linear operator, so it is possible to express any convolutional layer as a fully-connected layer.\n",
    "However, the convolutional layer's weight matrix is sparse (has many zeros) compared to a fully-connected (\"dense\") layer because each output only depends on a small number of inputs, namely, those within a small neigborhood. Further, the weight values are shared between the different pixel locations.\n",
    "\n",
    "This tutorial has some great visualisations and explanations on the details of conv layers: https://arxiv.org/abs/1603.07285.\n",
    "\n",
    "**Q:** Assuming a fixed input image size, do you think the reverse of the above also holds? Can any fully-connected layer be expressed as a convolutional layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e22f3054c49db1b0f317126be61b584b",
     "grade": true,
     "grade_id": "cell-a43655d6b36f378f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Technically, what's called a \"convolutional\" layer is usually implemented as a *cross-correlation* computation. Could there be any advantage in using the actual definition of convolution in these layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac4c1fc0f2dcb7473c7beaf33f315f92",
     "grade": true,
     "grade_id": "cell-9f6f8235182c9277",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the following simple CNN model. It may take about 15 minutes on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = models.Sequential([\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', \n",
    "                  kernel_initializer='he_uniform', padding='same', \n",
    "                  input_shape=image_shape),\n",
    "    layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "    layers.Conv2D(filters=64, kernel_size=3, activation='relu', \n",
    "                  kernel_initializer='he_uniform', padding='same'),\n",
    "    layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax')],\n",
    "    name='cnn')\n",
    "\n",
    "train_model(cnn, optimizer=optimizers.Adam, learning_rate=1e-3, n_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Does it improve the result? Does it run faster than the MLP? How many parameters does this model have?\n",
    "\n",
    "**Q:** How large is the output space of the first convolutional layer (i.e. how many numbers are output by that layer per image)? How does this compare to the size of the hidden layer in the MLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7357bc2d8c17bb971fff205157ec768",
     "grade": true,
     "grade_id": "cell-e6010db8bc2020df",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Forward Pass\n",
    "\n",
    "To confirm your understanding of the main CNN components, implement the forward pass of the convolutional, max pooling and dense layers, plus the relu and softmax activation functions. For simplicity, assume a fixed filter size of 3x3 for the convolution, with stride 1 and use zero padding, such that the spatial size does not change (called 'same' padding). Implement this in `conv3x3_same`. For max pooling assume a fixed 2x2 pooling size and stride 2 in `maxpool2x2`.\n",
    "\n",
    "To check whether your implementation is correct, we can extract the weights from the Keras model we trained above, and feed these weights and an test input to your implementation of the forward pass. If your result disagrees with Keras, there is probably a bug somewhere!\n",
    "\n",
    "You can also generalize these to other filter sizes and strides as well.\n",
    "(Implementation of the backward pass does not fit within this exercise, but the \"Machine Learning\" course of our chair does include such exercises.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e344cbb752636bb531121aad3a9e4489",
     "grade": true,
     "grade_id": "cell-e8926ce2153b13ca",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def conv3x3_same(x, weights, biases):\n",
    "    \"\"\"Convolutional layer with filter size 3x3 and 'same' padding.\n",
    "    `x` is a NumPy array of shape [height, width, n_features_in]\n",
    "    `weights` has shape [3, 3, n_features_in, n_features_out]\n",
    "    `biases` has shape [n_features_out]\n",
    "    Return the output of the 3x3 conv (without activation)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return result\n",
    "\n",
    "def maxpool2x2(x):\n",
    "    \"\"\"Max pooling with pool size 2x2 and stride 2.\n",
    "    `x` is a numpy array of shape [height, width, n_features]\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return result\n",
    "\n",
    "def dense(x, weights, biases):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "def relu(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def softmax(x):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def my_predict_cnn(x, W1, b1, W2, b2, W3, b3):\n",
    "    x = conv3x3_same(x, W1, b1)\n",
    "    x = relu(x)\n",
    "    x = maxpool2x2(x)\n",
    "    x = conv3x3_same(x, W2, b2)\n",
    "    x = relu(x)\n",
    "    x = maxpool2x2(x)\n",
    "    x = x.reshape(-1)\n",
    "    x = dense(x, W3, b3)\n",
    "    x = softmax(x)\n",
    "    return x\n",
    "\n",
    "W1, b1 = cnn.layers[0].get_weights()\n",
    "W2, b2 = cnn.layers[2].get_weights()\n",
    "W3, b3 = cnn.layers[5].get_weights()\n",
    "\n",
    "i_test = 12\n",
    "inp = x_test[i_test]\n",
    "my_prob = my_predict_cnn(inp, W1, b1, W2, b2, W3, b3)\n",
    "keras_prob = cnn.predict(inp[np.newaxis])[0]\n",
    "if np.mean((my_prob-keras_prob)**2) > 1e-10:\n",
    "    print('Something isn\\'t right! Keras gives different results than my_predict_cnn!')\n",
    "else:\n",
    "    print('Congratulations, you got correct results!')\n",
    "    \n",
    "i_maxpred = np.argmax(my_prob)\n",
    "plot_multiple([im_test[i_test]], [f'Pred: {labels[i_maxpred]}, {my_prob[i_maxpred]:.1%}'], imheight=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "Batch normalization is a modern technique to improve and speed up the training of deep neural networks (BatchNorm, Ioffe & Szegedy ICML'15, https://arxiv.org/abs/1502.03167). Each feature channel is normalized to have zero mean and unit variance across the spatial and mini-batch axes. To compensate for the lost degrees of freedom, extra scaling and bias parameters are introduced and learned. Mathematically, BatchNorm for a spatial feature map (e.g. the output of conv) can be written as:\n",
    "\n",
    "$$\n",
    "\\mu_d = \\mathbb{E}\\{x_{\\cdot \\cdot d}\\}, \\\\\n",
    "\\sigma_d = \\sqrt{\\operatorname{Var}\\{x_{\\cdot \\cdot d}\\}} \\\\\n",
    "z_{ijd} = \\gamma_d \\cdot \\frac{x_{ijd} - \\mu_d}{\\sigma_d} + \\beta_d,\\\\\n",
    "$$\n",
    "\n",
    "with the expectation and variance taken across both the data samples of the batch and the spatial dimensions.\n",
    "\n",
    "The $\\mu_d$ and $\\sigma_d$ values are computed on the actual mini-batch during training, but at test-time they are fixed, so that the prediction of the final system on a given sample does not depend on other samples in the mini-batch. To obtain the fixed values for test-time use, one needs to maintain moving statistics over the activations during training. This can be a bit tricky to implement from scratch, but luckily this is now implemented in all popular frameworks, including TensorFlow and Keras.\n",
    "\n",
    "**Q:** When applying BatchNorm, it is not necessary to use biases in the previous convolutional layer. Can you explain why? Use the \"use_bias\" argument of `layers.Conv2D` accordingly.\n",
    "\n",
    "**Q:** Furthermore, if the BatchNorm is followed by a linear or conv layer (with perhaps a ReLU in between), it is not necessary to use the to use the $\\gamma_d$ factor in BatchNorm (it can be turned off as `layers.BatchNormalization(scale=False)`). Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d13b60a4a3b4ad2c4dec7befb6164a1",
     "grade": true,
     "grade_id": "cell-0178a85974e9358e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a modified version of the previous model, where the `Conv2D` layers don't include the activation any more, and instead, insert a `layers.BatchNormalization()` and a `layers.Activation('relu')` layer after each conv. Does this model obtain better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d713f0e5e47039863a23dcf5c73209eb",
     "grade": true,
     "grade_id": "cell-83b754b10f9a5f09",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "cnn_batchnorm = models.Sequential([\n",
    "    # ....\n",
    "])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "train_model(cnn_batchnorm, optimizer=optimizers.Adam, learning_rate=1e-3, n_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strided Convolutions\n",
    "\n",
    "Max-pooling is a popular technique for reducing the spatial dimensionality\n",
    "of the outputs from conv layers. Another way to reduce dimensionality is striding. For an argument why this may be similarly effective, see [Springenberg et al., ICLRW'15](https://arxiv.org/pdf/1412.6806.pdf).\n",
    "\n",
    "Now create a model using the same architecture as before, with the difference of\n",
    "removing the max-pooling layers and increasing the stride parameter of the conv layers to $2 \\times 2$ in the spatial dimensions. \n",
    "\n",
    "What differences do you notice when training this new network?\n",
    "What is a clear advantage for using strides and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f475d92e4d3a8f2993801aa4e4a4c34b",
     "grade": true,
     "grade_id": "cell-34f5d6a1166b46fa",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "cnn_strides = models.Sequential([\n",
    "    # ....\n",
    "])\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "train_model(cnn_strides, optimizer=optimizers.Adam, learning_rate=1e-3, n_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Pooling\n",
    "\n",
    "The above network ends in a `Flatten` layer followed by a `Dense` layer, in which the number of weights depends on the input size. This means meaning that testing can only be performed on the exact same image size. Several architectures employ a (spatial) **global average pooling layer** to produce of vector of fixed size describing the whole image, instead of flattening.\n",
    "\n",
    "For this to work well, the units before the average pooling need to have a large enough receptive field. Therefore, compared with the previous model, remove the `Flatten` layer and instead add a third Conv-BatchNorm-ReLU combination, followed by a `layers.GlobalAveragePooling2D()` layer (before the final `Dense` layer).\n",
    "\n",
    "**Q:** Which network has more parameters, this or the previous one?\n",
    "\n",
    "**Q:** What is the size of the receptive field of the units in the layer directly before the global average pooling? (Remember: the receptive field of a particular unit (neuron) is the area of the *input image* that can influence the activation of this given unit).\n",
    "\n",
    "Train it and see if it reaches similar accuracy to the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "688c0018b5385bf8033928bbe8cb09a0",
     "grade": true,
     "grade_id": "cell-384e1eaafbd3f3b6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cnn_global_pool = models.Sequential([\n",
    "    # ....\n",
    "])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "train_model(cnn_global_pool, optimizer=optimizers.Adam, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Networks\n",
    "\n",
    "ResNet is a more modern architecture, introduced by He et al. in 2015 (published in 2016: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) and is still popular today.\n",
    "\n",
    "It consists of blocks like the following:\n",
    "\n",
    "![ResNet Block](resnet_block.png)\n",
    "\n",
    "Each of these so-called *residual blocks* only have to predict a *residual* (in plain words: the \"rest\", the \"leftover\") that will be added on top of its input.\n",
    "In other words, the block outputs how much each feature needs to be changed in order to enhance the representation compared to the previous block.\n",
    "\n",
    "There are several ways to combine residual blocks into *residual networks* (ResNets). In the following, we consider ResNet-v1, as used for the CIFAR-10 benchmark in the original ResNet paper (it is simpler compared to the full model that they used for the much larger ImageNet benchmark).\n",
    "\n",
    "Section 4.2. of the paper describes this architecture as follows: \"*The first layer is 3×3 convolutions. Then we use a stack of 6n layers with 3×3 convolutions on the feature maps of sizes {32, 16, 8} respectively, with 2n layers for each feature map size. The numbers of filters are {16, 32, 64} respectively. The subsampling is performed by convolutions with a stride of 2. The network ends with a global average pooling, a 10-way fully-connected layer, and softmax. [...] When shortcut connections are used, they are connected to the pairs of 3×3 layers (totally 3n shortcuts). On this dataset we use identity shortcuts in all cases.*\"\n",
    "\n",
    "Further, they use L2 regularization (a standard tool to combat overfitting). This penalizes weights with large magnitude by adding an additional term to the cost function, besides the cross-entropy. The overall function to optimize becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CE} + \\frac{\\lambda}{2} \\sum_{w\\in\\text{weights}} w^2,\n",
    "$$\n",
    "\n",
    "and in this paper $\\lambda=10^{-4}$.\n",
    "\n",
    "In the previous parts of this exercise we have already seen every major component we need to build this thing. However, ResNet is not a pure sequential architecture due to the skip connections. This means we cannot use `models.Sequential`. Luckily, Keras also offers a functional API. Look below to understand how this API works and fill in the missing pieces to make a ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8b0086de20d37a7f5e876a73f340721",
     "grade": true,
     "grade_id": "cell-78512b7752e9d7d2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def resnet(num_layers=56):\n",
    "    if (num_layers - 2) % 6 != 0:\n",
    "        raise ValueError('n_layers should be 6n+2 (eg 20, 32, 44, 56)')\n",
    "    n = (num_layers - 2) // 6\n",
    "        \n",
    "    inputs = layers.Input(shape=image_shape)\n",
    "    \n",
    "    # First layer\n",
    "    x = layers.Conv2D(16, 3, use_bias=False, \n",
    "        kernel_regularizer=regularizers.l2(1e-4),\n",
    "        padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    # Call the `resnet_block` function in loops to stack ResNet blocks according to the instructions above.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Global pooling and classifier on top\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = layers.Dense(10, activation='softmax',\n",
    "            kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "    return models.Model(inputs=inputs, outputs=outputs, name=f'resnet{num_layers}')\n",
    "\n",
    "def resnet_block(x, n_channels_out, strides=1):\n",
    "    # First conv\n",
    "    f = layers.Conv2D(n_channels_out, 3, strides, use_bias=False,\n",
    "            kernel_regularizer=regularizers.l2(1e-4),\n",
    "            padding='same', kernel_initializer='he_normal')(x)\n",
    "    f = layers.BatchNormalization(scale=False)(f)\n",
    "    f = layers.Activation('relu')(f)\n",
    "\n",
    "    # Second conv\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # The shortcut connection is just the identity.\n",
    "    # If feature channel counts differ between input and output,\n",
    "    # zero padding is used to match the depths.\n",
    "    # This is implemented by a Conv2D with fixed weights.\n",
    "    n_channels_in = x.shape[-1]\n",
    "    if n_channels_in != n_channels_out:\n",
    "        # Fixed weights, np.eye returns a matrix with 1s along the \n",
    "        # main diagonal and zeros elsewhere.\n",
    "        identity_weights = np.eye(n_channels_in, n_channels_out, dtype=np.float32)\n",
    "        layer = layers.Conv2D(\n",
    "            n_channels_out, kernel_size=1, strides=strides, use_bias=False, \n",
    "            kernel_initializer=initializers.Constant(value=identity_weights))\n",
    "        # Not learned! Set trainable to False:\n",
    "        layer.trainable = False\n",
    "        x = layer(x)\n",
    "       \n",
    "    # This is where the ResNet magic happens: the shortcut connection is\n",
    "    # added to the residual.\n",
    "    x = layers.add([x, f])\n",
    "    return layers.Activation('relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Decay and Data Augmentation - Our Final Model\n",
    "\n",
    "Learning rate decay reduces the learning rate as the training progresses. It can be implemented as a Keras callback as shown below.\n",
    "\n",
    "If you have a good GPU or lot of time, train ResNet-56 on the CIFAR-10 dataset for about 150 epochs. As a rough idea, it will take about 1-2 hours with a good GPU, but on a CPU it could take a day or two. If that's too long, train a smaller ResNet, wih `num_layers`=14 or 20, or do fewer epochs.\n",
    "\n",
    "To add data augmentation (e.g. random translation or rotation of the input images), look up the documentation for the `ImageDataGenerator` class. The ResNet model presented in the original paper was trained with random translations of $\\pm$ 4 px. Does the augmentation improve the final performance? What do you observe on the training and validation curves compared to no augmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23201076e421084806beeff4f6f131a8",
     "grade": true,
     "grade_id": "cell-f2042420b7d15963",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def learning_rate_schedule(epoch):\n",
    "    \"\"\"Learning rate is scheduled to be reduced after 80 and 120 epochs.\n",
    "    This function is automatically every epoch as part of callbacks\n",
    "    during training.\n",
    "    \"\"\"\n",
    "    if epoch < 80:\n",
    "        return 1e-3\n",
    "    if epoch < 120:\n",
    "        return 1e-4\n",
    "    return 1e-5\n",
    "\n",
    "def train_with_lr_decay(model):\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy', metrics=['accuracy'],\n",
    "        optimizer=optimizers.Adam(lr=1e-3))\n",
    "\n",
    "    # Callback for learning rate adjustment (see below)\n",
    "    lr_scheduler = callbacks.LearningRateScheduler(learning_rate_schedule)\n",
    "\n",
    "    # TensorBoard callback\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    logdir = os.path.join(log_root, f'{model.name}_{timestamp}')\n",
    "    tensorboard_callback = callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "    \n",
    "    # Fit the model on the batches generated by datagen.flow()\n",
    "    model.fit(\n",
    "        x_train, y_train, batch_size=128,\n",
    "        validation_data=(x_test, y_test), epochs=150, verbose=1, \n",
    "        callbacks=[lr_scheduler, tensorboard_callback])\n",
    "    \n",
    "def train_with_lr_decay_and_augmentation(model):\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', metrics=['accuracy'],\n",
    "        optimizer=optimizers.Adam(lr=1e-3))\n",
    "\n",
    "    # Callback for learning rate adjustment (see below)\n",
    "    lr_scheduler = callbacks.LearningRateScheduler(learning_rate_schedule)\n",
    "\n",
    "    # TensorBoard callback\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    logdir = os.path.join(log_root, f'{model.name}_augmented_{timestamp}')\n",
    "    tensorboard_callback = callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "    # Data augmentation: flip and shift horizontally/vertically by max 4 pixels\n",
    "    # datagen = kerasimage.ImageDataGenerator(...)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Note: model.fit with generator input seems to only work when the\n",
    "    # y targets are provided as one-hot vectors\n",
    "    y_train_onehot = tf.keras.utils.to_categorical(y_train, 10)\n",
    "    y_test_onehot = tf.keras.utils.to_categorical(y_test, 10)\n",
    "    \n",
    "    # Fit the model on the batches generated by datagen.flow() using model.fit()\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "resnet56 = resnet(56)\n",
    "train_with_lr_decay(resnet56)\n",
    "train_with_lr_decay_and_augmentation(resnet56)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
